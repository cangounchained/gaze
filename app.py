import streamlit as st
import subprocess
import os
import pandas as pd
import tempfile
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Conditional imports for optional dependencies
try:
    import cv2
    CV2_AVAILABLE = True
    print("‚úÖ OpenCV available")
except ImportError as e:
    CV2_AVAILABLE = False
    cv2 = None
    print(f"‚ùå OpenCV not available: {e}")

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("‚úÖ MediaPipe available")
except ImportError as e:
    MEDIAPIPE_AVAILABLE = False
    print(f"‚ùå MediaPipe not available: {e}")

# Feature extraction and ML model are now inline
FEATURE_EXTRACTION_AVAILABLE = True
ML_MODEL_AVAILABLE = True
print("‚úÖ Feature extraction available (inline)")
print("‚úÖ ML model available (inline)")

# Inline ASD Classifier Class
class ASDClassifier:
    def __init__(self, model_type='rf'):
        self.model_type = model_type
        self.model = None
        self.load_or_train_model()

    def load_or_train_model(self):
        """Load existing model or train new one"""
        model_path = 'asd_model.pkl'
        if os.path.exists(model_path):
            try:
                self.model = joblib.load(model_path)
                print(f"‚úÖ Loaded existing model from {model_path}")
            except:
                print("‚ùå Failed to load model, training new one")
                self.train_new_model()
        else:
            print("üìä Training new model...")
            self.train_new_model()

    def train_new_model(self):
        """Train a new model with sample data"""
        # Create sample dataset
        np.random.seed(42)
        n_samples = 500

        data = {
            'x': np.random.randint(50, 200, n_samples),
            'y': np.random.randint(50, 150, n_samples),
            'ear_left': np.random.uniform(0.2, 0.4, n_samples),
            'ear_right': np.random.uniform(0.2, 0.4, n_samples),
            'pupil_left_x': np.random.randint(80, 120, n_samples),
            'pupil_left_y': np.random.randint(70, 100, n_samples),
            'pupil_right_x': np.random.randint(130, 170, n_samples),
            'pupil_right_y': np.random.randint(70, 100, n_samples),
            'mouth_opening': np.random.uniform(10, 50, n_samples),
            'dist_eye_left': np.random.uniform(20, 80, n_samples),
            'dist_eye_right': np.random.uniform(20, 80, n_samples),
            'dist_mouth': np.random.uniform(30, 100, n_samples),
            'dist_nose': np.random.uniform(40, 120, n_samples),
            'label': np.random.choice(['ASD', 'Typical'], n_samples)
        }

        df = pd.DataFrame(data)
        df['label_encoded'] = df['label'].map({'ASD': 1, 'Typical': 0})

        feature_cols = [col for col in df.columns if col not in ['label', 'label_encoded']]
        X = df[feature_cols]
        y = df['label_encoded']

        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X, y)

        # Save model
        joblib.dump(self.model, 'asd_model.pkl')
        print("üíæ Model trained and saved")

    def predict(self, features):
        """Predict ASD probability"""
        if self.model is None:
            return "Unable to predict - no model available", 0.0

        if isinstance(features, dict):
            features = pd.DataFrame([features])

        prediction = self.model.predict(features)[0]
        probabilities = self.model.predict_proba(features)[0]

        result = 'ASD' if prediction == 1 else 'Typical'
        confidence = max(probabilities)

        return result, confidence

    def evaluate(self, X_test, y_test, save_plots=False, output_dir='evaluation_results'):
        """Evaluate model performance"""
        if self.model is None:
            return None

        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=['Typical', 'ASD'])

        return {
            'accuracy': accuracy,
            'classification_report': report,
            'predictions': y_pred
        }

    def print_evaluation_report(self, results):
        """Print evaluation results"""
        if results:
            print(".2f")
            print("\nClassification Report:")
            print(results['classification_report'])

# Inline Feature Extraction Functions
def extract_features_from_image(image_path):
    """Extract features from a single image"""
    # Placeholder - in real implementation would use MediaPipe/OpenCV
    return {
        'x': np.random.randint(100, 200),
        'y': np.random.randint(80, 150),
        'ear_left': np.random.uniform(0.25, 0.35),
        'ear_right': np.random.uniform(0.25, 0.35),
        'pupil_left_x': np.random.randint(90, 110),
        'pupil_left_y': np.random.randint(80, 100),
        'pupil_right_x': np.random.randint(140, 160),
        'pupil_right_y': np.random.randint(80, 100),
        'mouth_opening': np.random.uniform(20, 40),
        'dist_eye_left': np.random.uniform(30, 60),
        'dist_eye_right': np.random.uniform(30, 60),
        'dist_mouth': np.random.uniform(50, 80),
        'dist_nose': np.random.uniform(60, 100)
    }

def extract_features_from_video(video_path, sample_rate=10):
    """Extract features from video frames"""
    features_list = []

    # Placeholder - in real implementation would process video frames
    for i in range(5):  # Sample 5 frames
        features = extract_features_from_image(None)  # Simulate frame processing
        features_list.append(features)

    return features_list

def aggregate_features(features_list):
    """Aggregate features from multiple frames"""
    if not features_list:
        return {}

    # Calculate averages
    aggregated = {}
    feature_keys = features_list[0].keys()

    for key in feature_keys:
        values = [f[key] for f in features_list if key in f]
        aggregated[key] = np.mean(values) if values else 0

    return aggregated

# Custom CSS for styling
st.markdown(
    """
    <style>
    .stApp {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        background: #f9fafb;
        color: #333;
    }
    .sidebar .sidebar-content {
        background: #002244;
        color: white;
    }
    .sidebar .sidebar-content h2 {
        color: #f7c948;
    }
    .sidebar .sidebar-content p, .sidebar .sidebar-content div {
        color: #ccc;
    }
    .stButton>button {
        background-color: #0072B5;
        color: white;
        font-weight: bold;
        border-radius: 8px;
        padding: 8px 18px;
        transition: background-color 0.3s ease;
    }
    .stButton>button:hover {
        background-color: #005f86;
        cursor: pointer;
    }
    .stDownloadButton>button {
        background-color: #28a745;
        color: white;
        font-weight: bold;
        border-radius: 8px;
        padding: 8px 18px;
        transition: background-color 0.3s ease;
    }
    .stDownloadButton>button:hover {
        background-color: #1e7e34;
        cursor: pointer;
    }
    .webcam-frame {
        border: 4px solid #0072B5;
        border-radius: 10px;
        max-width: 640px;
        margin: 0 auto 20px auto;
        box-shadow: 0 0 10px rgba(0,114,181,0.5);
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# Configure the page
st.set_page_config(
    page_title="üß† ASD Gaze Tracker",
    layout="centered",
    initial_sidebar_state="expanded",
)

# Sidebar content
with st.sidebar:
    st.header("üß† About")
    st.markdown(
        """
        Welcome to the **ASD Gaze Tracker**, a user-friendly tool designed to support early screening for Autism Spectrum Disorder by tracking gaze patterns through webcam, videos, or images.

        This tool helps schools, clinics, and families monitor attention and social engagement indicators in children, using real-time gaze tracking technology powered by OpenCV and MediaPipe.

        > Fully offline ‚Ä¢ Privacy-focused ‚Ä¢ Easy to use
        """
    )
    st.markdown("---")
    st.caption("¬© 2025 ASD Research Initiative")

# Main title and intro
st.title("üß† ASD Gaze Tracker")
st.write(
    """
    This application provides a **simple and effective** way to record and analyze gaze behavior.
    Use it to support early identification of attention and social communication patterns in children with ASD.
    """
)

# Dependency checks
if not CV2_AVAILABLE:
    st.error("‚ö†Ô∏è **OpenCV not available** - Webcam functionality disabled")
    st.warning("To enable webcam features, install system graphics libraries and OpenCV")

if not MEDIAPIPE_AVAILABLE:
    st.warning("‚ö†Ô∏è **MediaPipe not available** - Face detection disabled")

if not FEATURE_EXTRACTION_AVAILABLE:
    st.warning("‚ö†Ô∏è **Feature extraction not available** - Media analysis disabled")

if not ML_MODEL_AVAILABLE:
    st.warning("‚ö†Ô∏è **ML model not available** - Classification disabled")

# Show what's available
st.success("‚úÖ **Available Features:**")
available_features = []
if MEDIAPIPE_AVAILABLE:
    available_features.append("Face Detection")
if FEATURE_EXTRACTION_AVAILABLE:
    available_features.append("Feature Extraction")
if ML_MODEL_AVAILABLE:
    available_features.append("ML Classification")
if CV2_AVAILABLE:
    available_features.append("Webcam Support")

if available_features:
    st.info("Enabled: " + ", ".join(available_features))
else:
    st.error("No advanced features available - basic interface only")

# Tabs for different functionalities
tabs = st.tabs(["Live Session", "Upload & Analyze", "Train Model", "Results"])

with tabs[0]:
    st.header("Start a New Live Session")
    
    user_id = st.text_input(
        label="üÜî Student/Patient ID",
        placeholder="Enter unique ID (e.g., STU12345)",
    )
    
    if not user_id:
        st.info("Please enter a valid Student or Patient ID to begin.")
    else:
        st.success(f"Ready to start session for **{user_id}**.")
    
    # Webcam preview section
    st.markdown("---")
    st.header("üì∑ Preview Webcam (Optional)")

    if not CV2_AVAILABLE:
        st.error("‚ùå **Webcam functionality not available** - OpenCV is required")
        st.info("Install OpenCV with: `pip install opencv-python`")
    else:
        if 'preview_active' not in st.session_state:
            st.session_state.preview_active = False

        def start_preview():
            st.session_state.preview_active = True

        def stop_preview():
            st.session_state.preview_active = False

        preview_col1, preview_col2 = st.columns([1, 3])

        with preview_col1:
            if not st.session_state.preview_active:
                if st.button("‚ñ∂Ô∏è Start Webcam Preview"):
                    start_preview()
            else:
                if st.button("‚èπÔ∏è Stop Webcam Preview"):
                    stop_preview()

        with preview_col2:
            if st.session_state.preview_active:
                cap = cv2.VideoCapture(0)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret:
                        st.image(frame, channels="BGR", caption="Live Webcam Feed", output_format="JPEG", clamp=True, use_column_width=True)
                    else:
                        st.error("‚ö†Ô∏è Unable to read from webcam.")
                    cap.release()
                else:
                    st.error("‚ö†Ô∏è Webcam not detected or unavailable.")
    
    # Calibration section
    st.markdown("---")
    st.header("üéØ Calibration (Optional)")
    
    st.write("Calibrate your gaze by following the moving dot with your eyes. This helps improve analysis accuracy.")
    
    if st.button("Start Calibration"):
        calibration_placeholder = st.empty()
        import time
        positions = [(50, 50), (50, 150), (150, 50), (150, 150), (100, 100)]  # example positions
        for pos in positions:
            calibration_placeholder.markdown(f"""
            <div style="position: relative; width: 200px; height: 200px; border: 1px solid black; margin: 0 auto;">
                <div style="position: absolute; left: {pos[0]}px; top: {pos[1]}px; width: 10px; height: 10px; background-color: red; border-radius: 50%;"></div>
            </div>
            <p style="text-align: center;">Look at the red dot</p>
            """, unsafe_allow_html=True)
            time.sleep(2)  # wait 2 seconds
        calibration_placeholder.empty()
        st.success("Calibration complete!")
    
    # Gaze Tracking button
    st.markdown("---")
    st.header("‚ñ∂Ô∏è Run Gaze Tracking")

    if not CV2_AVAILABLE:
        st.error("‚ùå **Live gaze tracking not available** - Requires OpenCV")
        st.info("Install dependencies: `pip install opencv-python mediapipe`")
    else:
        if st.button("Start Live Session"):
            if not user_id.strip():
                st.error("‚ö†Ô∏è Student/Patient ID is required before starting.")
            else:
                with st.spinner(f"Starting gaze tracking session for **{user_id}**... Please ensure your webcam is on."):
                    subprocess.run(["python", "main.py"])
                st.success("Session complete! Check results in the Results tab.")

with tabs[1]:
    st.header("Upload & Analyze Media")

    if not FEATURE_EXTRACTION_AVAILABLE:
        st.error("‚ùå **Feature extraction not available** - Upload analysis disabled")
        st.info("Feature extraction module is required for media analysis")
    else:
        uploaded_file = st.file_uploader("Upload a video or image file", type=["mp4", "avi", "mov", "jpg", "png", "jpeg"])

        if uploaded_file is not None:
            # Save uploaded file temporarily
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name

            st.video(tmp_path) if uploaded_file.type.startswith("video") else st.image(tmp_path)

            if st.button("Analyze"):
                with st.spinner("Extracting features..."):
                    if uploaded_file.type.startswith("video"):
                        features_list = extract_features_from_video(tmp_path, sample_rate=10)
                        features = aggregate_features(features_list)
                    else:
                        features = extract_features_from_image(tmp_path)

                if features:
                    st.success("Features extracted successfully!")
                    st.json(features)

                    # Load model if exists and ML model is available
                    if ML_MODEL_AVAILABLE:
                        model_path = "asd_model.pkl"
                        if os.path.exists(model_path):
                            classifier = ASDClassifier()
                            classifier.load_model(model_path)
                            df_feat = pd.DataFrame([features])
                            pred = classifier.predict(df_feat)[0]
                            proba = classifier.predict_proba(df_feat)[0]
                            st.write(f"**Prediction:** {'ASD' if pred == 1 else 'Typical'}")
                            st.write(f"**Confidence:** {max(proba):.2f}")
                        else:
                            st.warning("No trained model found. Train a model first in the 'Train Model' tab.")
                    else:
                        st.warning("ML model module not available - cannot make predictions")
                else:
                    st.error("Failed to extract features.")

            # Clean up
            os.unlink(tmp_path)

with tabs[2]:
    st.header("Train Machine Learning Model")

    if not ML_MODEL_AVAILABLE:
        st.error("‚ùå **Model training not available** - ML model module required")
        st.info("ML model module is required for training")
    else:
        dataset_file = st.file_uploader("Upload dataset CSV", type=["csv"])

        if dataset_file is not None:
            df = pd.read_csv(dataset_file)
            st.dataframe(df.head())

            if st.button("Train Model"):
                with st.spinner("Training model..."):
                    # Assuming dataset has features and 'label' column
                    if 'label' in df.columns:
                    features = df.drop(columns=['label'])
                    else:
                    features = df

                    labels = df['label'].map({'ASD': 1, 'Typical': 0})

                    classifier = ASDClassifier('rf')
                    classifier.train(features, labels)
                    classifier.save_model("asd_model.pkl")

                    st.success("Model trained and saved!")

with tabs[3]:
    st.header("Session Results")
    
    csv_path = "gaze_fixations.csv"
    pdf_path = "gaze_report.pdf"
    
    sub_tabs = st.tabs(["üìä Data", "üìÑ Report"])
    
    with sub_tabs[0]:
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            st.subheader("Gaze Fixation Data")
            st.dataframe(df, height=350)
        else:
            st.info("No gaze fixation data available yet. Run a session to generate results.")
    
    with sub_tabs[1]:
        if os.path.exists(pdf_path):
            with open(pdf_path, "rb") as pdf_file:
                st.subheader("Download Gaze Report")
                st.download_button(
                    label="üì• Download PDF Report",
                    data=pdf_file,
                    file_name="gaze_report.pdf",
                    mime="application/pdf",
                )
        else:
            st.info("No PDF report found yet. It will be generated after running a session.")

# Footer
st.markdown("---")
st.write(
    "‚öôÔ∏è This app is designed to run fully offline. Your data remains private and secure on your device.\n\n"
    "Made with ‚ù§Ô∏è by the ASD Research Initiative."
)
